{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9674c-f5c1-4c9f-a178-3d7278b3ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# --- Directories ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\n",
    "EXPERIMENTS_DIR = os.path.join(PROJECT_ROOT, \"experiments\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(EXPERIMENTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_frames(video_path: str, max_frames: int = 16) -> List[np.ndarray]:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Could not open {video_path}\")\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total <= 0:\n",
    "        cap.release()\n",
    "        return []\n",
    "    indices = np.linspace(0, total - 1, num=min(max_frames, total)).astype(int)\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frames.append(frame[:, :, ::-1])  # BGR->RGB\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, records: List[Dict[str, any]], max_frames: int = 16):\n",
    "        self.records = records\n",
    "        self.max_frames = max_frames\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.feature_extractor.eval()\n",
    "        for p in self.feature_extractor.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def _frames_to_features(self, frames: List[np.ndarray]) -> np.ndarray:\n",
    "        feats = []\n",
    "        for f in frames[:self.max_frames]:\n",
    "            t = self.preprocess(f)\n",
    "            with torch.no_grad():\n",
    "                feat = self.feature_extractor(t.unsqueeze(0)).squeeze().cpu().numpy()\n",
    "            feats.append(feat)\n",
    "        if len(feats) == 0:\n",
    "            feats = [np.zeros((512,))] * self.max_frames\n",
    "        if len(feats) < self.max_frames:\n",
    "            pad = [np.zeros_like(feats[0])] * (self.max_frames - len(feats))\n",
    "            feats.extend(pad)\n",
    "        return np.stack(feats, axis=0)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        rec = self.records[idx]\n",
    "        frames = extract_frames(rec[\"video_path\"], max_frames=self.max_frames)\n",
    "        feats = self._frames_to_features(frames)\n",
    "        return torch.tensor(feats, dtype=torch.float32), int(rec[\"label\"])\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, feat_dim: int = 512, hidden_size: int = 256,\n",
    "                 num_classes: int = 10, num_layers: int = 1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(feat_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn[-1])\n",
    "\n",
    "\n",
    "def train_video_baseline(train_records: List[Dict[str, any]], val_records: List[Dict[str, any]],\n",
    "                         num_classes: int = 10, device: str = \"cpu\",\n",
    "                         experiment_name: str = \"exp_video\") -> Dict[str, List[float]]:\n",
    "    train_ds = VideoFrameDataset(train_records)\n",
    "    val_ds = VideoFrameDataset(val_records)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "    model = LSTMClassifier(num_classes=num_classes).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(1, 6):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for feats, labels in train_loader:\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            out = model(feats)\n",
    "            loss = loss_fn(out, labels)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running += loss.item()\n",
    "        history[\"train_loss\"].append(running / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for feats, labels in val_loader:\n",
    "                feats, labels = feats.to(device), labels.to(device)\n",
    "                preds = model(feats).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        history[\"val_acc\"].append(acc)\n",
    "        print(f\"Epoch {epoch} train loss {history['train_loss'][-1]:.4f}, val acc {acc:.4f}\")\n",
    "\n",
    "    # --- Save plots ---\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_loss\"], marker=\"o\", label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_acc\"], marker=\"s\", label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.title(\"Video Classification Training\")\n",
    "    plt.legend()\n",
    "    plot_path = os.path.join(RESULTS_DIR, f\"{experiment_name}_plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Save experiment log ---\n",
    "    exp_record = {\n",
    "        \"experiment\": experiment_name,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"params\": {\"epochs\": 5, \"lr\": 1e-3, \"device\": device},\n",
    "        \"history\": history\n",
    "    }\n",
    "    log_path = os.path.join(EXPERIMENTS_DIR, f\"{experiment_name}_log.json\")\n",
    "    with open(log_path, \"w\") as f:\n",
    "        json.dump(exp_record, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Results plot saved to {plot_path}\")\n",
    "    print(f\"✅ Experiment log saved to {log_path}\")\n",
    "    return exp_record\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Run train_video_baseline with prepared train/val records.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
